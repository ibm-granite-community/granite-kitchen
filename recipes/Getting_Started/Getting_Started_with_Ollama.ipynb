{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using inference calls against a model hosted locally on [Ollama](https://ollama.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
    "    langchain_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an LLM on Ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ollama server can be run locally on your system, or wherever the notebook is run (eg., [Colab](https://colab.research.google.com/)).\n",
    "\n",
    "### Option A: Run Ollama Locally (Linux, MacOS, Windows)\n",
    "\n",
    "1. [Download and install Ollama](https://ollama.com/download), if you haven't already.\n",
    "1. Start the Ollama server: `ollama serve`\n",
    "1. Pull down Granite models:\n",
    "\n",
    "    ```shell\n",
    "    ollama pull ibm/granite4:micro\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run Ollama in the Notebook Environment\n",
    "\n",
    "1. Download and install Ollama, if you haven't already:\n",
    "\n",
    "    ```shell\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If the Ollama server was not started by the install script, start the Ollama server using `nohup` and `&`; this will run the server independently in the background:\n",
    "\n",
    "    ```shell\n",
    "    !nohup ollama serve &\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pull down Granite models:\n",
    "\n",
    "    ```shell\n",
    "    !ollama pull ibm/granite4:micro\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the LLM with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Select one of the models you pulled with Ollama above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm/granite4:micro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "Invoke the model with a prompt, and get an answer back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell a story about a duck who likes french fries\")\n",
    "\n",
    "response = model.invoke(prompt.format_prompt())\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
