{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using inference calls against a model hosted locally on [Ollama](https://ollama.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/granite-kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an LLM on Ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ollama server can be run locally on your system, or wherever the notebook is run (eg., [Colab](https://colab.research.google.com/)).\n",
    "\n",
    "### Option A: Run Ollama Locally (Linux, MacOS, Windows)\n",
    "\n",
    "1. [Download and install Ollama](https://ollama.com/download), if you haven't already.\n",
    "1. Start the Ollama server: `ollama serve`\n",
    "1. Pull down Granite models:\n",
    "\n",
    "    ```shell\n",
    "    ollama pull granite3-dense:2b\n",
    "    ollama pull granite3-dense:8b\n",
    "    ollama pull granite-code:3b\n",
    "    ollama pull granite-code:8b\n",
    "    ollama pull granite-code:20b\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run Ollama in the Notebook Environment\n",
    "\n",
    "1. Download and install Ollama, if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start the Ollama server using `nohup` and `&`; this will run the server independently in the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"nohup ollama serve &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pull down Granite models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull granite3-dense:2b\n",
    "# !ollama pull granite3-dense:8b\n",
    "# !ollama pull granite-code:3b\n",
    "# !ollama pull granite-code:8b\n",
    "# !ollama pull granite-code:20b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the LLM with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Select one of the models you pulled with Ollama above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"granite3-dense:2b\"\n",
    "# model_id = \"granite3-dense:8b\"\n",
    "# model_id = \"granite-code:3b\"\n",
    "# model_id = \"granite-code:8b\"\n",
    "# model_id = \"granite-code:20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "Invoke the model with a prompt, and get an answer back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "<|start_of_role|>user<|end_of_role|>\\\n",
    "Tell a story about a duck who likes french fries.<|end_of_text|>\n",
    "<|start_of_role|>assistant<|end_of_role|>\"\"\"\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
