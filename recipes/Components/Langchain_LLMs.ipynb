{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs with Langchain\n",
    "\n",
    "In this recipe, we show how to obtain a model client for different providers using Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "> **NOTE**: When running this recipe in [Colab](https://colab.research.google.com/), you may see an error about dependency conflicts with `google-colab 1.0.0`. You can safely ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    langchain_community \\\n",
    "    replicate \\\n",
    "    langchain_ollama \\\n",
    "    langchain-ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate\n",
    "\n",
    "For instructions on how to set up Replicate, see [Getting Started with Replicate](../Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To select a Granite model, go to the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-3.2-8b-instruct\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "To use Ollama in Granite Cookbook recipes, you will need to run the Ollama server locally to where Jupyter notebook is running, either on your own system or in Colab. \n",
    "\n",
    "For instructions on how to set up and run Ollama, see [Getting Started with Ollama](../Getting_Started/Getting_Started_with_Ollama.ipynb).\n",
    "\n",
    "To select a Granite Code model, go to the [Ollama granite code library](https://ollama.com/library/granite-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"granite3.2:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WatsonX\n",
    "\n",
    "For instructions on how to set up WatsonX, see [Getting Started with WatsonX](../Getting_Started/Getting_Started_with_WatsonX.ipynb).\n",
    "\n",
    "To select a Granite model, go to the [list of Foundation Models](https://www.ibm.com/products/watsonx-ai/foundation-models) on WatsonX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "    url= get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
